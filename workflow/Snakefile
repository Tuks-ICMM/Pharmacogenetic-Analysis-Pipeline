from pandas import read_csv, Series
from os.path import join
from snakemake.utils import validate
from snakemake.utils import min_version
__author__ = "Graeme Ford"
__credits__ = [
    "Graeme Ford",
    "Prof. Michael S. Pepper",
    "Prof. Fourie Joubert",
    "Fatima Barmania",
    "Megan Holborn",
]
__version__ = "1.0.0"
__maintainer__ = "Graeme Ford"
__email__ = "graeme.ford@tuks.co.za"
__status__ = "Development"

# Enforce version check
min_version("7.24.2")


# Import and validate manifest file:
configfile: join("config", "manifest.json")
validate(config, join("..", "config", ".schema", "manifest.schema.json"))

# SET REPORT TEMPLATE
# report: "report/template.rst"

locations = read_csv(join(*config["input"]["locations"]), header=0)
samples = read_csv(join(*config["input"]["samples"]), header=0)
datasets = read_csv(join(*config["input"]["datasets"]), header=0)
transcripts = read_csv(join(*config["input"]["transcripts"]), header=0)


# DEFINE CONTEXT-VARIABLES:
clusters = set([cluster for cluster in samples.keys() if cluster not in ["sample_name", "dataset"]])

include: "rules/common.smk"

# [IMPORT] VCF-Validation-Workflow and override local rules with non-local input from theVCF-Validation-Workflow
include: "rules/import_vcf_validation_workflow.smk"

# [IMPORT] Population Structure Workflow
include: "rules/import_population_structure_workflow.smk"

# [IMPORT] Custom functions to connect checkpoints with variable outputs
include: "rules/checkpoint_connectors.smk"

# DEFINE CONTAINERIZED ENVIRONMENT:
# container: "docker://graemeford/pipeline-os"

# Wildcard constraints (RegEx)
location_names_allowed=r"[a-zA-Z0-9\-]+"
cluster_names_allowed=r"[a-zA-Z0-9\-]+"


rule format_sample_metadata:
    log: out("tmp/formatted_sample_metadata/All.log")
    benchmark: out("tmp/formatted_sample_metadata/All.benchmark")
    conda:
        join("envs", "pharmacogenetic-analysis-workflow.yml")
    input:
        sample_annotations="input/samples.csv"
    output:
        sample_metadata=out("tmp/formatted_sample_metadata/samples.tsv")
    script:
        join("scripts", "format_sample_metadata.py")


rule tabix:
    log: out("tmp/{output}/{operation}.log")
    benchmark: out("tmp/{output}/{operation}.benchmark")
    wildcard_constraints:
        output=r"[a-zA-Z0-9\-]+",
        operation=r"[a-zA-Z0-9\-\_]+"
    input:
        out("tmp/{output}/{operation}.vcf.gz")
    output:
        out("tmp/{output}/{operation}.vcf.gz.tbi")
    shell:
        """
        tabix -p vcf {input}
        """


rule merge_datasets:
    log: out("tmp/{contig}/merged_datasets.log")
    benchmark: out("tmp/{contig}/merged_datasets.benchmark")
    wildcard_constraints: # TODO: Make this configurable
        contig=r"[0-9]{1,2}",
    input:
        # vcf=lambda wildcards: vcfValidationWorkflowAdapter(".vcf.gz", wildcards),
        vcf=expand(out("tmp/{{contig}}/{dataset}/sorted_variant_records.vcf.gz"), dataset=datasets["dataset_name"].unique().tolist()),
        # vcfIndex=lambda wildcards: vcfValidationWorkflowAdapter(".vcf.gz.tbi", wildcards),
        vcfIndex=expand(out("tmp/{{contig}}/{dataset}/sorted_variant_records.vcf.gz.tbi"), dataset=datasets["dataset_name"].unique().tolist()),
    output:
         out("tmp/{contig}/merged_datasets.vcf.gz")
    shell:
        """
        bcftools merge -O z -o {output} {input.vcf}
        """


rule normalize_merged_datasets:
    log: out("tmp/{contig}/normalized_merged_datasets.log")
    benchmark: out("tmp/{contig}/normalized_merged_datasets.benchmark")
    wildcard_constraints: # TODO: Make this configurable
        contig=r"[0-9]{1,2}"
    input:
        out("tmp/{contig}/merged_datasets.vcf.gz")
    output:
        out("tmp/{contig}/normalized_merged_datasets.vcf.gz")
    shell:
        """
        bcftools norm --multiallelics -any -Oz -o {output} {input}
        """


rule convert_to_pgen:
    log: out("tmp/{contig}/converted_to_pgen.log")
    benchmark: out("tmp/{contig}/converted_to_pgen.benchmark")
    wildcard_constraints: # TODO: Make this configurable
        contig=r"[0-9]{1,2}"
    params:
        output=lambda wildcards, output: output["pgen"].replace(".pgen", "")
    input: 
        vcf=lambda wildcards: out("tmp/{contig}/normalized_merged_datasets.vcf.gz") if datasets.shape[0] >1 else vcfValidationWorkflowAdapter(".vcf.gz"),
        vcfIndex=lambda wildcards: out("tmp/{contig}/merged_datasets.vcf.gz.tbi") if datasets.shape[0] >1 else vcfValidationWorkflowAdapter(".vcf.gz.tbi"),
        sample_metadata=out("tmp/formatted_sample_metadata/samples.tsv")
    output: 
        pgen=out("tmp/{contig}/converted_to_pgen.pgen"),
        pvar=out("tmp/{contig}/converted_to_pgen.pvar.zst"),
        psam=out("tmp/{contig}/converted_to_pgen.psam"),
    threads: workflow.cores * 0.25
    shell:
        """
        plink2 --threads {threads} --vcf {input.vcf} --update-sex {input.sample_metadata} --split-par hg38 --allow-extra-chr --make-pgen vzs --out {params.output} >{log} 2>&1
        """


rule verify_records_against_reference_genome:
    log: out("tmp/{contig}/verified_records_against_reference_genome.log")
    benchmark: out("tmp/{contig}/verified_records_against_reference_genome.benchmark")
    wildcard_constraints: # TODO: Make this configurable
        contig=r"[0-9]{1,2}"
    params:
        input=lambda wildcards, input: input["pgen"].replace(".pgen", ""),
        output=lambda wildcards, output: output["pgen"].replace(".pgen", ""),
        ref=lambda wildcards: join(
        *next(
            i["path"]
            for i in config["resources"]["reference_genomes"]
        if i["name"] == "GRCh38"
            ),
        )
    input: 
        pgen=out("tmp/{contig}/converted_to_pgen.pgen"),
        pvar=out("tmp/{contig}/converted_to_pgen.pvar.zst"),
        psam=out("tmp/{contig}/converted_to_pgen.psam")
    output: 
        pgen=out("tmp/{contig}/verified_records_against_reference_genome.pgen"),
        pvar=out("tmp/{contig}/verified_records_against_reference_genome.pvar.zst"),
        psam=out("tmp/{contig}/verified_records_against_reference_genome.psam"),
    threads: workflow.cores * 0.25
    shell:
        """
        plink2 --threads {threads} --pfile {params.input} vzs --fa {params.ref} --ref-from-fa force --allow-extra-chr --make-pgen vzs --out {params.output} >{log} 2>&1
        """


rule remove_non_standard_chromosomes:
    log: out("tmp/{contig}/removed_non_standard_chromosomes.log")
    benchmark: out("tmp/{contig}/removed_non_standard_chromosomes.benchmark")
    wildcard_constraints: # TODO: Make this configurable
        contig=r"[0-9]{1,2}"
    params:
        input=lambda wildcards, input: input["pgen"].replace(".pgen", ""),
        output=lambda wildcards,output: output["pgen"].replace(".pgen", "")
    input:
        pgen=out("tmp/{contig}/verified_records_against_reference_genome.pgen"),
        pvar=out("tmp/{contig}/verified_records_against_reference_genome.pvar.zst"),
        psam=out("tmp/{contig}/verified_records_against_reference_genome.psam"),
    output:
        pgen=out("tmp/{contig}/removed_non_standard_chromosomes.pgen"),
        pvar=out("tmp/{contig}/removed_non_standard_chromosomes.pvar.zst"),
        psam=out("tmp/{contig}/removed_non_standard_chromosomes.psam")
    threads: workflow.cores * 0.25
    shell:
        """
        plink2 --threads {threads} --pfile {params.input} vzs --allow-extra-chr --output-chr chr26 --chr 1-26 --make-pgen vzs --out {params.output} >{log} 2>&1
        """


rule remove_unknown_samples:
    log: out("tmp/{contig}/removed_unknown_samples.log")
    benchmark: out("tmp/{contig}/removed_unknown_samples.benchmark")
    wildcard_constraints: # TODO: Make this configurable
        contig=r"[0-9]{1,2}"
    params:
        input=lambda wildcards, input: input["pgen"].replace(".pgen", ""),
        output=lambda wildcards, output: output["pgen"].replace(".pgen", ""),
        samples=lambda wildcards, input: ",".join(samples["sample_name"].tolist())
    input:
        pgen=out("tmp/{contig}/removed_non_standard_chromosomes.pgen"),
        pvar=out("tmp/{contig}/removed_non_standard_chromosomes.pvar.zst"),
        psam=out("tmp/{contig}/removed_non_standard_chromosomes.psam"),
        sample_metadata=out("tmp/formatted_sample_metadata/samples.tsv")
    output:
        pgen=out("tmp/{contig}/removed_unknown_samples.pgen"),
        pvar=out("tmp/{contig}/removed_unknown_samples.pvar.zst"),
        psam=out("tmp/{contig}/removed_unknown_samples.psam"),
    threads: workflow.cores * 0.25
    shell:
        """
        plink2 --threads {threads} --pfile {params.input} vzs --keep {input.sample_metadata} --make-pgen vzs --out {params.output} >{log} 2>&1
        """


rule filter_variant_missingness:
    log: out("tmp/{contig}/filtered_variant_missingness.log")
    benchmark: out("tmp/{contig}/filtered_variant_missingness.benchmark")
    wildcard_constraints: # TODO: Make this configurable
        contig=r"[0-9]{1,2}"
    params:
        input=lambda wildcards, input: input["pgen"].replace(".pgen", ""),
        output=lambda wildcards, output: output["pgen"].replace(".pgen", "")
    input: 
        pgen=out("tmp/{contig}/removed_unknown_samples.pgen"),
        pvar=out("tmp/{contig}/removed_unknown_samples.pvar.zst"),
        psam=out("tmp/{contig}/removed_unknown_samples.psam"),
    output:
        pgen=out("tmp/{contig}/filtered_variant_missingness.pgen"),
        pvar=out("tmp/{contig}/filtered_variant_missingness.pvar.zst"),
        psam=out("tmp/{contig}/filtered_variant_missingness.psam"),
    threads: workflow.cores * 0.25
    shell: 
        """
        plink2 --threads {threads} --pfile {params.input} vzs --geno {config[parameters][variant-missingness-cutoff]} --make-pgen vzs --out {params.output} >{log} 2>&1
        """


rule filter_sample_missingness:
    log: out("tmp/{contig}/filtered_sample_missingness.log")
    benchmark: out("tmp/{contig}/filtered_sample_missingness.benchmark")
    wildcard_constraints: # TODO: Make this configurable
        contig=r"[0-9]{1,2}"
    params:
        input=lambda wildcards, input: input["pgen"].replace(".pgen", ""),
        output=lambda wildcards, output: output["pgen"].replace(".pgen", "")
    input:
        pgen=out("tmp/{contig}/filtered_variant_missingness.pgen"),
        pvar=out("tmp/{contig}/filtered_variant_missingness.pvar.zst"),
        psam=out("tmp/{contig}/filtered_variant_missingness.psam"),
    output:
        pgen=out("tmp/{contig}/filtered_sample_missingness.pgen"),
        pvar=out("tmp/{contig}/filtered_sample_missingness.pvar.zst"),
        psam=out("tmp/{contig}/filtered_sample_missingness.psam"),
    threads: workflow.cores * 0.25
    shell: 
        """
        plink2 --threads {threads} --pfile {params.input} vzs --mind {config[parameters][sample-missingness-cutoff]} --make-pgen vzs --out {params.output} >{log} 2>&1
        """


rule calculate_sample_relatedness:
    log: out("tmp/{contig}/calculated_sample_relatedness.log")
    benchmark: out("tmp/{contig}/calculated_sample_relatedness.benchmark")
    wildcard_constraints: # TODO: Make this configurable
        contig=r"[0-9]{1,2}"
    params:
        input=lambda wildcards, input: input["pgen"].replace(".pgen", ""),
        output=lambda wildcards, output: output["inclusion_list"].replace(".king.cutoff.in.id", "")
    input:
        pgen=out("tmp/{contig}/filtered_sample_missingness.pgen"),
        pvar=out("tmp/{contig}/filtered_sample_missingness.pvar.zst"),
        psam=out("tmp/{contig}/filtered_sample_missingness.psam"),
    output:
        inclusion_list=out("tmp/{contig}/calculated_sample_relatedness.king.cutoff.in.id"),
        exclusion_list=out("tmp/{contig}/calculated_sample_relatedness.king.cutoff.out.id")
    threads: workflow.cores * 0.25
    shell:
        """
        plink2 --threads {threads} --pfile {params.input} vzs --king-cutoff {config[parameters][kingship-cutoff]} --out {params.output} >{log} 2>&1
        """


rule remove_related_samples:
    log: out("tmp/{contig}/removed_related_samples.log")
    benchmark: out("tmp/{contig}/removed_related_samples.benchmark")
    wildcard_constraints: # TODO: Make this configurable
        contig=r"[0-9]{1,2}"
    params:
        input=lambda wildcards, input: input["pgen"].replace(".pgen", ""),
        output=lambda wildcards, output: output["pgen"].replace(".pgen", "")
    input:
        pgen=out("tmp/{contig}/filtered_sample_missingness.pgen"),
        pvar=out("tmp/{contig}/filtered_sample_missingness.pvar.zst"),
        psam=out("tmp/{contig}/filtered_sample_missingness.psam"),
        unrelated_samples=out("tmp/{contig}/calculated_sample_relatedness.king.cutoff.in.id"),
    output:
        pgen=out("tmp/{contig}/removed_related_samples.pgen"),
        pvar=out("tmp/{contig}/removed_related_samples.pvar.zst"),
        psam=out("tmp/{contig}/removed_related_samples.psam"),
    threads: workflow.cores * 0.25
    shell:
        """
        plink2 --threads {threads} --pfile {params.input} vzs --keep {input.unrelated_samples} --make-pgen vzs --out {params.output} >{log} 2>&1
        """


rule extract_provided_coordinates:
    log: out("tmp/{location}/extracted_provided_coordinates.log"),
    benchmark: out("tmp/{location}/extracted_provided_coordinates.benchmark")
    wildcard_constraints:
        location=location_names_allowed
    params:
        fromBP=lambda wildcards: locations.loc[locations["location_name"] == wildcards.location, "start"].item(),
        toBP=lambda wildcards: locations.loc[locations["location_name"] == wildcards.location, "stop"].item(),
        chr=lambda wildcards: locations.loc[locations["location_name"] == wildcards.location, "chromosome"].item(),
        input=lambda wildcards, input: input["pgen"].replace('.pgen', ""),
        output=lambda wildcards, output: output["pgen"].replace(".pgen", ""),
    input:
        pgen=lambda wildcards: out(f"tmp/{locations.loc[locations["location_name"] == wildcards.location, "chromosome"].item()}/removed_related_samples.pgen"),
        pvar=lambda wildcards: out(f"tmp/{locations.loc[locations["location_name"] == wildcards.location, "chromosome"].item()}/removed_related_samples.pvar.zst"),
        psam=lambda wildcards: out(f"tmp/{locations.loc[locations["location_name"] == wildcards.location, "chromosome"].item()}/removed_related_samples.psam"),
        sample_metadata=out("tmp/formatted_sample_metadata/samples.tsv")
    output:
        pgen=out("tmp/{location}/extracted_provided_coordinates.pgen"),
        pvar=out("tmp/{location}/extracted_provided_coordinates.pvar.zst"),
        psam=out("tmp/{location}/extracted_provided_coordinates.psam"),
    threads: workflow.cores * 0.25
    shell:
        """
        plink2 --threads {threads} --pfile {params.input} vzs --pheno {input.sample_metadata} --from-bp {params.fromBP} --to-bp {params.toBP} --chr {params.chr} --make-pgen vzs --out {params.output} >{log} 2>&1
        """

checkpoint report_count_partitioned_per_cluster:
    log: out("tmp/{cluster}/{location}/reported_frequency_per_cluster/allele_count.log"),
    benchmark: out("tmp/{cluster}/{location}/reported_frequency_per_cluster/allele_count.benchmark")
    params:
        input=lambda wildcards, input: input["pgen"].replace(".pgen", ""),
        output=lambda wildcards: out(f"tmp/{wildcards.cluster}/{wildcards.location}/reported_frequency_per_cluster/allele_count"),
    input:
        pgen=out("tmp/{location}/extracted_provided_coordinates.pgen"),
        pvar=out("tmp/{location}/extracted_provided_coordinates.pvar.zst"),
        psam=out("tmp/{location}/extracted_provided_coordinates.psam")
    output:
        files=directory(out("tmp/{cluster}/{location}/reported_frequency_per_cluster/"))
    wildcard_constraints:
        location=location_names_allowed,
        cluster=cluster_names_allowed,
    threads: workflow.cores * 0.25
    shell:
        """
        plink2 --threads {threads} --pfile {params.input} vzs --loop-cats {wildcards.cluster} --freq counts cols=chrom,pos,ref,alt,reffreq,altfreq,nobs --out {params.output} >{log} 2>&1
        """


checkpoint report_hardy_weinberg_per_cluster:
    log: out("tmp/{cluster}/{location}/hardy_weinberg_per_cluster/hardy_weinberg.log"),
    benchmark: out("tmp/{cluster}/{location}/hardy_weinberg_per_cluster/hardy_weinberg.benchmark")
    params:
        input= lambda wildcards, input: input["pgen"].replace(".pgen", ""),
        output=lambda wildcards: out(f"tmp/{wildcards.cluster}/{wildcards.location}/hardy_weinberg_per_cluster/hardy_weinberg")
    wildcard_constraints:
        cluster=cluster_names_allowed,
        location=location_names_allowed
    input:
        pgen=out("tmp/{location}/extracted_provided_coordinates.pgen"),
        pvar=out("tmp/{location}/extracted_provided_coordinates.pvar.zst"),
        psam=out("tmp/{location}/extracted_provided_coordinates.psam"),
    output:
        directory=directory(out("tmp/{cluster}/{location}/hardy_weinberg_per_cluster/")),
    threads: workflow.cores * 0.25
    shell:
        """
        plink2 --threads {threads} --pfile {params.input} vzs --loop-cats {wildcards.cluster} --hardy midp cols=chrom,pos,ref,alt,gcounts,hetfreq,p --out {params.output} >{log} 2>&1
        """


checkpoint report_missingness_per_cluster:
    log: out("tmp/{cluster}/{location}/missingness_per_cluster/missingness.log")
    benchmark: out("tmp/{cluster}/{location}/missingness_per_cluster/missingness.benchmark")
    params:
        input=lambda wildcards, input: input["pgen"].replace(".pgen", ""),
        output=lambda wildcards: out(f"tmp/{wildcards.cluster}/{wildcards.location}/missingness_per_cluster/missingness")
    wildcard_constraints:
        cluster=cluster_names_allowed,
        location=location_names_allowed
    input:
        pgen=out("tmp/{location}/extracted_provided_coordinates.pgen"),
        pvar=out("tmp/{location}/extracted_provided_coordinates.pvar.zst"),
        psam=out("tmp/{location}/extracted_provided_coordinates.psam"),
    output:
        missingness_reports=directory(out("tmp/{cluster}/{location}/missingness_per_cluster/"))
    threads: workflow.cores * 0.25
    shell:
        """
        plink2 --threads {threads} --pfile {params.input} vzs --loop-cats {wildcards.cluster} --missing zs vcols=chrom,pos,ref,alt,provref,nmiss,nobs,fmiss --out {params.output} >{log} 2>&1
        """


rule collect_variant_count:
    log: out("tmp/{cluster}/{location}/variant_count.log")
    benchmark: out("tmp/{cluster}/{location}/variant_count.benchmark")
    wildcard_constraints:
        location=location_names_allowed,
        cluster=cluster_names_allowed,
    threads: workflow.cores * 0.25
    input:
        pvar=out("tmp/{location}/extracted_provided_coordinates.pvar.zst"),
        allele_counts=collect_report_count_partitioned_per_cluster,
    output:
        allele_counts=out("tmp/{cluster}/{location}/variant_count.csv")
    script:
        join("scripts", "collect_variant_count.py")


rule collect_variant_frequency:
    log: out("tmp/{cluster}/{location}/variant_frequency.log")
    benchmark: out("tmp/{cluster}/{location}/variant_frequency.benchmark")
    wildcard_constraints:
        location=location_names_allowed,
        cluster=cluster_names_allowed,
    threads: workflow.cores * 0.25
    input:
        pvar=out("tmp/{location}/extracted_provided_coordinates.pvar.zst"),
        psam=out("tmp/{location}/extracted_provided_coordinates.psam"),
        allele_counts=out("tmp/{cluster}/{location}/variant_count.csv"),
    output:
        file=out("tmp/{cluster}/{location}/variant_frequency.csv")
    script:
        join("scripts", "collect_variant_frequency.py")


rule report_fishers_exact_with_corrections:
    log: out("tmp/{cluster}/{location}/fishers_exact.log")
    benchmark: out("tmp/{cluster}/{location}/fishers_exact.benchmark")
    params:
        reference_population=lambda wildcards: config["parameters"]["fishers-test"][wildcards.cluster],
    wildcard_constraints:
        location=location_names_allowed,
        cluster=cluster_names_allowed,
    threads: workflow.cores * 0.25
    input:
        psam=out("tmp/{location}/extracted_provided_coordinates.psam"),
        allele_counts=out("tmp/{cluster}/{location}/variant_count.csv"),
    output:
        out("tmp/{cluster}/{location}/fishers_exact_with_corrections.csv")
    script:
        join("scripts", "report_fishers_exact_with_corrections.py")


rule collect_autosomal_hardy_weinberg:
    log: out("tmp/{cluster}/{location}/autosomal_hardy_weinberg.log")
    benchmark: out("tmp/{cluster}/{location}/autosomal_hardy_weinberg.benchmark")
    wildcard_constraints:
        location=location_names_allowed,
        cluster=cluster_names_allowed,
    threads: workflow.cores * 0.25
    input:
        pvar=out("tmp/{location}/extracted_provided_coordinates.pvar.zst"),
        hardy_weinberg=collect_autosomal_hardy_weinberg_per_cluster,
    output:
        out("tmp/{cluster}/{location}/autosomal_hardy_weinberg.csv")
    script:
        join("scripts", "collect_autosomal_hardy_weinberg.py")


rule collect_variant_missingness:
    log: out("tmp/{cluster}/{location}/missingnessPartitionedPerCluster.log")
    benchmark: out("tmp/{cluster}/{location}/missingnessPartitionedPerCluster.benchmark")
    wildcard_constraints:
        location=location_names_allowed,
        cluster=cluster_names_allowed,
    threads: workflow.cores * 0.25
    input:
        pvar=out("tmp/{location}/extracted_provided_coordinates.pvar.zst"),
        missingness_reports=collect_report_missingness_per_cluster,
    output:
        variant_missingness_report=out("tmp/{cluster}/{location}/missingness.csv")
    script:
        join("scripts", "collect_variant_missingness.py")


rule query_variant_effect_predictions:
    log: out("tmp/{cluster}/{location}/variant_effect_predictions.log"),
    benchmark: out("tmp/{cluster}/{location}/variant_effect_predictions.benchmark")
    wildcard_constraints:
        location=location_names_allowed,
        cluster=cluster_names_allowed,
    threads: workflow.cores * 0.25
    params:
        strand=lambda wildcards: locations.loc[locations["location_name"] == wildcards.location, "strand"]
    input:
        pvar=out("tmp/{location}/extracted_provided_coordinates.pvar.zst"),
    output:
        out("tmp/{cluster}/{location}/variant_effect_predictions.csv")
    script:
        join("scripts", "query_variant_effects.py")


rule compile_variant_effect_predictions:
    log: out("tmp/{cluster}/{location}/cleaned_variant_effect_predictions.log"),
    benchmark: out("tmp/{cluster}/{location}/cleaned_variant_effect_predictions.benchmark")
    wildcard_constraints:
        location=location_names_allowed,
        cluster=cluster_names_allowed,
    threads: workflow.cores * 0.25
    input:
        vep_results=out("tmp/{cluster}/{location}/variant_effect_predictions.csv")
    output:
        cleaned_vep_results=out("tmp/{cluster}/{location}/cleaned_variant_effect_predictions.csv")
    script:
        join("scripts", "compile_variant_effects.py")


rule consolidate_reports:
    log: out("consolidated_reports/{cluster}_{location}.log")
    benchmark: out("consolidated_reports/{cluster}_{location}.benchmark")
    wildcard_constraints:
        location=location_names_allowed,
        cluster=cluster_names_allowed,
    threads: workflow.cores * 0.25
    input:
        pvar=out("tmp/{location}/extracted_provided_coordinates.pvar.zst"),
        analyses=collect_reports_to_consolidate,
    output:
        consolidated_data=out("consolidated_reports/{cluster}_{location}.csv")
    script:
        join("scripts", "consolidate_reports.py")

print(rules.population_structure_all.input)
rule all:
    default_target: True
    log:
        "logs/ALL/ALL.log",
    input:
        # Contig-level data
        expand(out("tmp/{contig}/merged_datasets.vcf.gz"), contig=locations["chromosome"].unique().tolist()),
        expand(out("tmp/{contig}/normalized_merged_datasets.vcf.gz"), contig=locations["chromosome"].unique().tolist()),
        expand(out("tmp/{contig}/verified_records_against_reference_genome.{ext}"), contig=locations["chromosome"].unique().tolist(), ext=["pgen", "pvar.zst", "psam"]),
        expand(out("tmp/{contig}/removed_non_standard_chromosomes.{ext}"), contig=locations["chromosome"].unique().tolist(), ext=["pgen", "pvar.zst", "psam"]),
        expand(out("tmp/{contig}/removed_unknown_samples.{ext}"), contig=locations["chromosome"].unique().tolist(), ext=["pgen", "pvar.zst", "psam"]),
        expand(out("tmp/{contig}/filtered_variant_missingness.{ext}"), contig=locations["chromosome"].unique().tolist(), ext=["pgen", "pvar.zst", "psam"]),
        expand(out("tmp/{contig}/filtered_sample_missingness.{ext}"), contig=locations["chromosome"].unique().tolist(), ext=["pgen", "pvar.zst", "psam"]),
        expand(out("tmp/{contig}/calculated_sample_relatedness.{ext}"), contig=locations["chromosome"].unique().tolist(), ext=["king.cutoff.in.id", "king.cutoff.out.id"]),
        expand(out("tmp/{contig}/removed_related_samples.{ext}"), contig=locations["chromosome"].unique().tolist(), ext=["pgen", "pvar.zst", "psam"]),
        
        # Location-level data
        expand(out("tmp/{contig}/extracted_provided_coordinates.{ext}"), contig=locations["location_name"].unique().tolist(), ext=["pgen", "pvar.zst", "psam"]),

        # Reports
        expand(out("tmp/{cluster}/{location}/variant_count.csv"), cluster=clusters, location=locations["location_name"]),
        expand(out("tmp/{cluster}/{location}/variant_frequency.csv"), cluster=clusters, location=locations["location_name"]),
        expand(out("tmp/{cluster}/{location}/fishers_exact_with_corrections.csv"), cluster=config["parameters"]["fishers-test"].keys(), location=locations["location_name"]),
        expand(out("tmp/{cluster}/{location}/autosomal_hardy_weinberg.csv"), cluster=clusters, location=locations["location_name"]),
        expand(out("tmp/{cluster}/{location}/missingness.csv"), cluster=clusters, location=locations["location_name"]),
        expand(out("tmp/{cluster}/{location}/variant_effect_predictions.csv"), cluster=clusters, location=locations["location_name"]),
        expand(out("tmp/{cluster}/{location}/cleaned_variant_effect_predictions.csv"), cluster=clusters, location=locations["location_name"]),
        
        expand(out("consolidated_reports/{cluster}_{location}.csv"), cluster=clusters, location=locations["location_name"]),
        rules.population_structure_all.input
